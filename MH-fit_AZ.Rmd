---
title: "MH Fit Assessment"
author: "Adina Zhang"
date: "May 10, 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Continuing off of the MH algorithm by Steven

```{r}
set.seed(24)
chains = mh(train_Y, train_X, start_b = rep(2,7), 
            start_r = 0.6, start_s = 10, window_b = rep(3,7),
            window_r = 0.02, window_s = 150, n = 10000)
```

## Unique values

```{r}
length(unique(chains[,2]))
length(unique(chains[,3]))
length(unique(chains[,4]))
length(unique(chains[,5]))
length(unique(chains[,6]))
length(unique(chains[,7]))
length(unique(chains[,8]))
length(unique(chains[,9]))
length(unique(chains[,10]))
```


## Visualization of posterior distributions and chains

Based off of visualizations, it seems that a small burn-in of about 200 or 300 iterations will allow the chain to stabilize. Visualizations are based off of n = 5000.

```{r}
# Visualize posteriors and chains
chains_dat = chains %>% as.data.frame()
chains_dat %>% 
  ggplot(aes(x = V1, y = V10)) +
  geom_line() +
  labs(x = "Index",
       y = "Variance")

chains_dat %>% 
  ggplot(aes(x = V1, y = V9)) +
  geom_line() + 
  labs(x = "Index", y = "Rho")

chains_dat %>% 
  ggplot(aes(x = V1, y = V2)) +
  geom_line() + 
  labs(x = "Index", y = "Intercept")

chains_dat %>% 
  ggplot(aes(x = V1, y = V4)) +
  geom_line() + 
  labs(x = "Index", y = "Beta3")
```

```{r}
# Visualize posterior distributions
chains_dat %>% 
  ggplot(aes(x = V2)) + 
  geom_histogram() + 
  labs(x = "Intercept")

chains_dat %>% 
  ggplot(aes(x = V10)) + 
  geom_histogram() + 
  labs(x = "Variance")

chains_dat %>% 
  ggplot(aes(x = V9)) + 
  geom_histogram() +
  labs(x = "Rho") + 
  theme_bw()

chains_dat %>% 
  ggplot(aes(x = V4)) + 
  geom_histogram() +
  labs(x = "Beta3")

chains_dat %>% 
  ggplot(aes(x = V6)) + 
  geom_histogram() +
  labs(x = "Beta5")
```

## Apply burn-in

```{r}
# Remove the first 2000 chain values
burn_in_chains = chains_dat[10001:dim(chains_dat)[1],]
```

```{r}
# Visualize posteriors and chains
burn_in_chains %>% 
  ggplot(aes(x = V1, y = V10)) +
  geom_line() +
  labs(x = "Index",
       y = "Variance")

burn_in_chains %>% 
  ggplot(aes(x = V1, y = V9)) +
  geom_line() + 
  labs(x = "Index", y = "Rho")

burn_in_chains %>% 
  ggplot(aes(x = V1, y = V2)) +
  geom_line() + 
  labs(x = "Index", y = "Intercept")

burn_in_chains %>% 
  ggplot(aes(x = V1, y = V4)) +
  geom_line() + 
  labs(x = "Index", y = "Beta3")

burn_in_chains %>% 
  ggplot(aes(x = V1, y = V6)) +
  geom_line() + 
  labs(x = "Index", y = "Beta5")
```

```{r}
# Visualize posterior distributions
burn_in_chains %>% 
  ggplot(aes(x = V2)) + 
  geom_histogram() + 
  labs(x = "Intercept")

burn_in_chains %>% 
  ggplot(aes(x = V10)) + 
  geom_histogram() + 
  labs(x = "Variance")

burn_in_chains %>% 
  ggplot(aes(x = V9)) + 
  geom_histogram() + 
  labs(x = "Rho")

burn_in_chains %>% 
  ggplot(aes(x = V4)) + 
  geom_histogram() +
  labs(x = "Beta3")

burn_in_chains %>% 
  ggplot(aes(x = V6)) + 
  geom_histogram() +
  labs(x = "Beta5")
```

## Prediction and Inference

```{r}
# Take average of coefficients
beta_avg = burn_in_chains %>% summarise_at(c("V2", "V3", "V4", "V5", "V6", "V7", "V8"), 
                                           mean, na.rm = T)
rho_avg = burn_in_chains %>% summarise_at("V9", mean, na.rm = T)
sigma_avg = burn_in_chains %>% summarise_at("V10", mean, na.rm = T)

# 95% CI
beta_avg_l95 = burn_in_chains %>% summarise_at(c("V2", "V3", "V4", "V5", "V6", "V7", "V8"), 
                                               quantile, 0.025, na.rm = T)
beta_avg_u95 = burn_in_chains %>% summarise_at(c("V2", "V3", "V4", "V5", "V6", "V7", "V8"), 
                                               quantile, 0.975, na.rm = T)

rho_avg_l95 = burn_in_chains %>% summarise_at("V9", quantile, 0.025, na.rm = T)
rho_avg_u95 = burn_in_chains %>% summarise_at("V9", quantile, 0.975, na.rm = T)

sig_avg_l95 = burn_in_chains %>% summarise_at("V10", quantile, 0.025, na.rm = T)
sig_avg_u95 = burn_in_chains %>% summarise_at("V10", quantile, 0.975, na.rm = T)

# Table of parameters
par_avg = cbind(beta_avg, rho_avg, sigma_avg) %>% round(digits = 2)
par_l95 = cbind(beta_avg_l95, rho_avg_l95, sig_avg_l95) %>% round(digits = 2)
par_u95 = cbind(beta_avg_u95, rho_avg_u95, sig_avg_u95) %>% round(digits = 2)
parameters = c("Intercept", "Day of Year", "Year", "Nature/Type", 
               "Diff Lat", "Diff Long", "Diff Wind", "Rho", "Sigma")

par_table = cbind(parameters, t(par_avg), t(par_l95), t(par_u95)) %>% as.data.frame()
names(par_table) = c("Parameters", "Posterior Mean", "L95%", "U95%")
row.names(par_table) = c()
write_csv(par_table, "table_parameters.csv")

# Predict wind speed at time t
par = unlist(c(beta_avg, rho_avg))

# Test data
test_X = dt %>%
  filter(!(ID %in% train_hurr)) %>%
  mutate(intercept = 1) %>%
  select(intercept, day_of_yr, year, Nature, 
         diff_lat, diff_long, diff_spd, Wind.kt)

test_Y = dt %>%
  filter(!(ID %in% train_hurr)) %>%
  select(Wind.t6)

test.fit = data.matrix(test_X) %*% par
test.res = test_Y - test.fit

# Pull IDs from original data
test.hurricanes = dt %>%
  filter(!(ID %in% train_hurr)) %>%
  select(ID)
# Data set with Predicted and Accurate values
hurricane_predict = tibble(
  id = test.hurricanes,
  test_Y = test_Y,
  pred_Y = test.fit
)

preds <- cbind(id = test.hurricanes, test_X, true = test_Y, preds = test.fit, res = test.res)
write.csv(preds, "./preds2.csv")
```

